{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install yolov8"
      ],
      "metadata": {
        "id": "4IgIoPDTvjiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.196\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "0XFarYchvgLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3f310f-6def-4c12-84c2-4e9ff5ae1dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 30.2/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ],
      "metadata": {
        "id": "BZRYo7gtvgN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Roboflow Supervision"
      ],
      "metadata": {
        "id": "3-8xlh_wwSEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import supervision as sv\n",
        "print(\"supervision.__version__:\", sv.__version__)"
      ],
      "metadata": {
        "id": "MXML0kP6vgQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef04a98-ff58-4f5e-c1c8-3c5b53722e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "supervision.__version__: 0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prediction of model"
      ],
      "metadata": {
        "id": "-ONn5YEewI4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Function to downsample video\n",
        "def downsample_video(input_path, output_path, scale_factor):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # Get the video properties\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (int(frame_width / scale_factor), int(frame_height / scale_factor)))\n",
        "\n",
        "    # Read until video is completed\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            # Downsample the frame\n",
        "            downsized_frame = cv2.resize(frame, (int(frame_width / scale_factor), int(frame_height / scale_factor)))\n",
        "            # Write the downsized frame to the output video file\n",
        "            out.write(downsized_frame)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Release the video capture and writer objects\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Input and output file paths\n",
        "input_video_path = '/content/drive/MyDrive/Copy of Start to End Hyderabad Part 3 720P.mp4'\n",
        "output_video_path = '/content/drive/MyDrive/downsampled_video.mp4'\n",
        "\n",
        "# Define the downsampling factor\n",
        "downsampling_factor = 2  # Adjust this according to your needs\n",
        "\n",
        "# Call the function to downsample the video\n",
        "downsample_video(input_video_path, output_video_path, downsampling_factor)\n",
        "\n",
        "print(\"Video downsampling complete.\")\n",
        "\n",
        "# Load YOLO model\n",
        "model = YOLO('/content/drive/MyDrive/Road_furniture_weights/content/runs/detect/train/weights/best.pt')\n",
        "\n",
        "# Define callback function to be used in video processing\n",
        "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "    # Model prediction on single frame and conversion to supervision Detections\n",
        "    results = model(frame, verbose=False)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "    # Annotate the frame with all detected classes\n",
        "    labels = [f\"{model.model.names[class_id]} {confidence:0.2f}\"\n",
        "              for confidence, class_id in zip(detections.confidence, detections.class_id)]\n",
        "\n",
        "    bbox_annotator = sv.BoundingBoxAnnotator(thickness=3)\n",
        "    label_annotator = sv.LabelAnnotator(text_thickness=2, text_scale=2)  # Adjust text_thickness and text_scale to reduce font size\n",
        "\n",
        "    annotated_frame = bbox_annotator.annotate(scene=frame.copy(), detections=detections)\n",
        "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
        "    return annotated_frame\n",
        "\n",
        "# Process the whole video\n",
        "SOURCE_VIDEO_PATH = '/content/drive/MyDrive/downsampled_video.mp4'\n",
        "TARGET_VIDEO_PATH = '/content/drive/MyDrive/Prediction_Start to End Hyderabad Part 3_road_furniture.mp4'\n",
        "\n",
        "sv.process_video(\n",
        "    source_path=SOURCE_VIDEO_PATH,\n",
        "    target_path=TARGET_VIDEO_PATH,\n",
        "    callback=callback\n",
        ")\n",
        "\n",
        "print(\"Video processing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPk0xMKIcXSt",
        "outputId": "87ec9b5e-f568-4d59-94b6-24755c794c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video downsampling complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brfxznW1cjSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}